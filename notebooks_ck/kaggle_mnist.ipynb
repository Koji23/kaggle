{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home = './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display(mnist)\n",
    "#display(mnist.data)\n",
    "#display(mnist.target)\n",
    "display(mnist.data.shape)\n",
    "display(mnist.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mnist.data\n",
    "target = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1f756e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[0,:].reshape([28,28]), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples, n_pix = data.shape\n",
    "n_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350000, 784)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "dataL = np.zeros([n_samples*5,n_pix])\n",
    "targetL = np.zeros([n_samples*5])\n",
    "index=0\n",
    "for i in range(n_samples):\n",
    "    for j in range(-2,3):\n",
    "        #im = Image.fromarray(np.uint8(data[0,:].reshape([28,28])))\n",
    "        dataL[index,:] = np.array( Image.fromarray(data[i,:].reshape([28,28])).rotate(j*5) ).ravel()\n",
    "        targetL[index]=target[i]\n",
    "        index +=1\n",
    "        \n",
    "#im = Image.fromarray(data[0,:].reshape([28,28]))\n",
    "#imAr = np.array( im.transform(im.size, Image.AFFINE, (1, 1, 0, 1, 1, 0)) ).ravel()\n",
    "\n",
    "display(dataL.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2a18f048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data.ravel(), bins =256)\n",
    "plt.ylim([0,1e6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a33f0d1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(target, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a30d37b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "data, target = shuffle(data,target, random_state=0)\n",
    "#dataL, targetL = shuffle(dataL,targetL, random_state=0)\n",
    "\n",
    "\n",
    "plt.plot(target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6300, 784)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(dataL, targetL, test_size=0.1, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[0:n_samples//10,:], target[0:n_samples//10], test_size=0.1, random_state=42)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 8., 6.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.predict(X_test[0:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.predict_proba(X_test[0:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9731428571428572"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(70, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(70,30))\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 8., 6.])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(X_test[0:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9654285714285714"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test,y_test)\n",
    "#got 0.958 right out of the box with MLPclassifier\n",
    "#got 0.970 with augmentation of plus minus 2*10degree of rotating\n",
    "#got 0.966 with hidden_layer_sizes=(100,20,)   , no augmentation\n",
    "#for much smaller dataset / 10   I get 0.91 on default and only 0.75 for hidden_layer_sizes=(100,20,) (not enough data to train right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow basics\n",
    "http://adventuresinmachinelearning.com/python-tensorflow-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable a is [[ 3.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [15.]\n",
      " [18.]\n",
      " [21.]\n",
      " [24.]\n",
      " [27.]\n",
      " [30.]]\n"
     ]
    }
   ],
   "source": [
    "const = tf.constant(2.0, name='const')\n",
    "#b = tf.Variable(2.0, name='b')\n",
    "b = tf.placeholder(tf.float32, [None, 1], name='b')\n",
    "c = tf.Variable(1.0, name='c')\n",
    "d = tf.add(b,c, name='d')\n",
    "e = tf.add(c, const, name='e')\n",
    "a = tf.multiply(d,e, name='a')\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    #a_out=sess.run(a)\n",
    "    a_out = sess.run(a, feed_dict={b: np.arange(0, 10)[:, np.newaxis]})\n",
    "    print('Variable a is {}'.format(a_out))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow solution with tensorflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 1 cost = 0.600\n",
      "Epoch: 2 cost = 0.217\n",
      "0.9645\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.5\n",
    "epochs = 2\n",
    "batch_size = 100\n",
    "\n",
    "n_L0 = 784\n",
    "n_L1 = 300\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None,n_L0])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_L0, n_L1], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([n_L1]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([n_L1, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "\n",
    "L1_x = tf.add(tf.matmul(x, W1), b1)\n",
    "L1_z = tf.nn.relu(L1_x)\n",
    "\n",
    "L2_x = tf.add(tf.matmul(L1_z, W2), b2)\n",
    "y_ = tf.nn.softmax(L2_x)\n",
    "\n",
    "y_clipped = tf.clip_by_value(y_ , 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)+ (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "   # initialise the variables\n",
    "   sess.run(init_op)\n",
    "   total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "   for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "   print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow with original data from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "n_L0 = 784\n",
    "n_L1 = 300\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_L0])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "#yV = tf.placeholder(tf.float32, [None])\n",
    "#y = tf.one_hot(tf.cast(yV,tf.int32),10)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_L0, n_L1], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([n_L1]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([n_L1, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "\n",
    "L1_x = tf.add(tf.matmul(x, W1), b1)\n",
    "L1_z = tf.nn.relu(L1_x)\n",
    "\n",
    "L2_x = tf.add(tf.matmul(L1_z, W2), b2)\n",
    "y_ = tf.nn.softmax(L2_x)\n",
    "\n",
    "y_clipped = tf.clip_by_value(y_ , 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)+ (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 0.684\n",
      "Epoch: 2 cost = 0.245\n",
      "Epoch: 3 cost = 0.178\n",
      "Epoch: 4 cost = 0.143\n",
      "Epoch: 5 cost = 0.114\n",
      "Epoch: 6 cost = 0.096\n",
      "Epoch: 7 cost = 0.077\n",
      "Epoch: 8 cost = 0.068\n",
      "Epoch: 9 cost = 0.057\n",
      "Epoch: 10 cost = 0.048\n",
      "0.9677143\n"
     ]
    }
   ],
   "source": [
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    total_batch = int(len(y_train) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x = X_train[i*batch_size:(i+1)*batch_size,:]/256\n",
    "            #batch_y = tf.one_hot(y_train[i*batch_size:(i+1)*batch_size],10)\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y_one_hot = np.zeros([batch_size,10])\n",
    "            for j in range(len(batch_y1)):\n",
    "                batch_y_one_hot[j,int(batch_y[j])]=1\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y_one_hot})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "    \n",
    "    y_test_one_hot = np.zeros([len(y_test),10])\n",
    "    for j in range(len(y_test)):\n",
    "        y_test_one_hot[j,int(y_test[j])]=1\n",
    "    print(sess.run(accuracy, feed_dict={x: X_test, y: y_test_one_hot}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 3 layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "n_L0 = 784\n",
    "n_L1 = 50\n",
    "n_L2 = 20\n",
    "n_L3 = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_L0])\n",
    "#y = tf.placeholder(tf.float32, [None, 10])\n",
    "yV = tf.placeholder(tf.float32, [None])\n",
    "y = tf.one_hot(tf.cast(yV,tf.int32),10)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_L0, n_L1], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([n_L1]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([n_L1, n_L2], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([n_L2]), name='b2')\n",
    "W3 = tf.Variable(tf.random_normal([n_L2, n_L3], stddev=0.03), name='W3')\n",
    "b3 = tf.Variable(tf.random_normal([n_L3]), name='b3')\n",
    "\n",
    "L1_x = tf.add(tf.matmul(x, W1), b1)\n",
    "L1_z = tf.nn.relu(L1_x)\n",
    "\n",
    "L2_x = tf.add(tf.matmul(L1_z, W2), b2)\n",
    "L2_z = tf.nn.relu(L2_x)\n",
    "\n",
    "L3_x = tf.add(tf.matmul(L2_z, W3), b3)\n",
    "y_ = tf.nn.softmax(L3_x)\n",
    "\n",
    "y_clipped = tf.clip_by_value(y_ , 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)+ (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 1.044\n",
      "Epoch: 2 cost = 0.388\n",
      "Epoch: 3 cost = 0.313\n",
      "Epoch: 4 cost = 0.279\n",
      "Epoch: 5 cost = 0.253\n",
      "Epoch: 6 cost = 0.237\n",
      "Epoch: 7 cost = 0.220\n",
      "Epoch: 8 cost = 0.210\n",
      "Epoch: 9 cost = 0.204\n",
      "Epoch: 10 cost = 0.192\n",
      "0.89114285\n"
     ]
    }
   ],
   "source": [
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    total_batch = int(len(y_train) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x = X_train[i*batch_size:(i+1)*batch_size,:]/256\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            for j in range(len(batch_y1)):\n",
    "                batch_y_one_hot[j,int(batch_y[j])]=1\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, yV: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "    \n",
    "    y_test_one_hot = np.zeros([len(y_test),10])\n",
    "    for j in range(len(y_test)):\n",
    "        y_test_one_hot[j,int(y_test[j])]=1\n",
    "    print(sess.run(accuracy, feed_dict={x: X_test, yV: y_test}))\n",
    "    \n",
    "#maybe too many parameters and we need to train longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow simple 2 layer use layers.dense and built in entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "n_L0 = 784\n",
    "n_L1 = 300\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_L0])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "L1_z = tf.layers.dense(inputs=x,    units=300, activation=tf.nn.relu)\n",
    "L2_x = tf.layers.dense(inputs=L1_z, units=10)\n",
    "\n",
    "y_ = tf.nn.softmax(L2_x)\n",
    "\n",
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=tf.argmax(input=y, axis=1), logits=L2_x)\n",
    "    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 0.252\n",
      "Epoch: 2 cost = 0.102\n",
      "Epoch: 3 cost = 0.070\n",
      "Epoch: 4 cost = 0.051\n",
      "Epoch: 5 cost = 0.039\n",
      "Epoch: 6 cost = 0.030\n",
      "Epoch: 7 cost = 0.023\n",
      "Epoch: 8 cost = 0.018\n",
      "Epoch: 9 cost = 0.014\n",
      "Epoch: 10 cost = 0.011\n",
      "0.97885716\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(len(y_train) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x = X_train[i*batch_size:(i+1)*batch_size,:]/256\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y_one_hot = np.zeros([batch_size,10])\n",
    "            for j in range(len(batch_y1)):\n",
    "                batch_y_one_hot[j,int(batch_y[j])]=1\n",
    "            _, c = sess.run([optimizer, cross_entropy], feed_dict={x: batch_x, y: batch_y_one_hot})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "    \n",
    "    y_test_one_hot = np.zeros([len(y_test),10])\n",
    "    for j in range(len(y_test)):\n",
    "        y_test_one_hot[j,int(y_test[j])]=1\n",
    "    print(sess.run(accuracy, feed_dict={x: X_test, y: y_test_one_hot}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow convolution nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "n_L0 = 784\n",
    "n_L1 = 300\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_L0])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#L1_z = tf.layers.dense(inputs=x,    units=300, activation=tf.nn.relu)\n",
    "#L2_x = tf.layers.dense(inputs=L1_z, units=10)\n",
    "\n",
    "input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=input_layer, filters=32,\n",
    "      kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu, name='conv1')\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=pool1, filters=64,\n",
    "      kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64]) # first dim corresponds to batch size\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "logits = tf.layers.dense(inputs=dense, units=10) #note there is no activation\n",
    "\n",
    "y_ = tf.nn.softmax(logits)\n",
    "\n",
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=tf.argmax(input=y, axis=1), logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy) # i forgot this line\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 2.300\n",
      "0.15142857\n",
      "<tf.Variable 'conv2d_1/kernel:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n",
      "[[[[-0.03074797  0.01592069 -0.02542857 ... -0.02137831  0.0014137\n",
      "    -0.03625771]\n",
      "   [-0.0066529   0.04070442  0.02103827 ...  0.04097736  0.02872676\n",
      "    -0.0301507 ]\n",
      "   [ 0.03069934  0.01814351 -0.03701154 ...  0.00846366  0.02895211\n",
      "     0.03040886]\n",
      "   ...\n",
      "   [-0.00533701 -0.04990847 -0.04977497 ...  0.03946779  0.01464431\n",
      "    -0.02657025]\n",
      "   [ 0.04319182  0.04981729  0.00917548 ...  0.04818431  0.01019125\n",
      "     0.00136767]\n",
      "   [-0.03508659  0.03369844  0.04722837 ... -0.04032509  0.03498507\n",
      "     0.03777622]]\n",
      "\n",
      "  [[ 0.0416056   0.03404805  0.02996466 ...  0.03193381  0.04108379\n",
      "     0.01260222]\n",
      "   [-0.01174397  0.01120044 -0.00803977 ... -0.03763349 -0.0044721\n",
      "    -0.01898385]\n",
      "   [ 0.01631466 -0.04062457 -0.04468497 ...  0.03457603 -0.01581925\n",
      "     0.02261167]\n",
      "   ...\n",
      "   [ 0.01261698 -0.04797035 -0.04645151 ...  0.03776478  0.00614528\n",
      "    -0.04251678]\n",
      "   [-0.04759227 -0.01375394 -0.0015832  ... -0.04260815  0.04115773\n",
      "     0.0447419 ]\n",
      "   [-0.00683179 -0.02565258  0.03861106 ... -0.01851331 -0.01120086\n",
      "    -0.04914824]]\n",
      "\n",
      "  [[ 0.01479623 -0.04736935 -0.02277796 ...  0.03707539  0.01137554\n",
      "     0.0013093 ]\n",
      "   [-0.04936485 -0.04295672  0.04580244 ... -0.00650371 -0.03899249\n",
      "    -0.02810458]\n",
      "   [ 0.04004229 -0.04356055 -0.0386724  ...  0.0051885   0.03972591\n",
      "     0.01522176]\n",
      "   ...\n",
      "   [ 0.01805303 -0.00368649 -0.03514578 ...  0.00117624 -0.02633054\n",
      "     0.01375382]\n",
      "   [ 0.0416736   0.01860745 -0.00378398 ... -0.01783184  0.04749701\n",
      "    -0.01335361]\n",
      "   [ 0.01181238  0.00299593  0.03262994 ... -0.0008952  -0.03522073\n",
      "    -0.00427244]]\n",
      "\n",
      "  [[ 0.02141981 -0.01860601 -0.01580564 ... -0.04030124 -0.01185417\n",
      "    -0.01812132]\n",
      "   [ 0.02012011 -0.04878367 -0.01546527 ...  0.02860362  0.02521468\n",
      "     0.01952653]\n",
      "   [ 0.00467516  0.01770866 -0.008075   ...  0.03658781 -0.00358551\n",
      "    -0.00564788]\n",
      "   ...\n",
      "   [ 0.04360652 -0.01041378  0.03801589 ... -0.04451421 -0.01436094\n",
      "     0.02715397]\n",
      "   [-0.04251572 -0.04824449  0.03085842 ... -0.03957163  0.00492469\n",
      "     0.01258362]\n",
      "   [ 0.03846321 -0.00010868 -0.02916553 ...  0.03137875  0.03291218\n",
      "     0.00033839]]\n",
      "\n",
      "  [[-0.00763084  0.02417577 -0.04219056 ... -0.01281401  0.04449536\n",
      "     0.03881376]\n",
      "   [-0.01256621  0.04910997 -0.04733638 ...  0.01915092  0.01865287\n",
      "     0.04455983]\n",
      "   [ 0.01383493  0.0206216   0.01781065 ... -0.03168371 -0.0255156\n",
      "     0.02094309]\n",
      "   ...\n",
      "   [ 0.03857904  0.03502837 -0.00645186 ...  0.03276915 -0.00877068\n",
      "    -0.02620627]\n",
      "   [-0.04057686 -0.02347298 -0.04249593 ... -0.04699681  0.00285023\n",
      "     0.0284799 ]\n",
      "   [-0.02029189  0.01445179 -0.0249637  ... -0.03409748 -0.00065224\n",
      "    -0.03834895]]]\n",
      "\n",
      "\n",
      " [[[ 0.01758181  0.04110208 -0.04475856 ...  0.02605608 -0.02232127\n",
      "    -0.00495799]\n",
      "   [-0.0189934  -0.04943785 -0.04457725 ...  0.03746046 -0.04126177\n",
      "    -0.02372533]\n",
      "   [ 0.01656816 -0.01053386 -0.02775104 ... -0.00866906  0.04320444\n",
      "    -0.0276243 ]\n",
      "   ...\n",
      "   [-0.00482607 -0.00630301 -0.03906747 ... -0.000888    0.01092536\n",
      "    -0.02067598]\n",
      "   [ 0.02175634 -0.01578028 -0.03486717 ...  0.00583208 -0.01675729\n",
      "    -0.04805765]\n",
      "   [-0.04297824  0.01292052  0.03835985 ... -0.02776504  0.04807368\n",
      "    -0.03492653]]\n",
      "\n",
      "  [[ 0.00062597  0.00960995 -0.00482628 ... -0.03216562  0.02405069\n",
      "     0.03417853]\n",
      "   [ 0.0224095  -0.01651084 -0.02995974 ...  0.00144035 -0.00829626\n",
      "     0.03502257]\n",
      "   [-0.0005736   0.04459501 -0.00410658 ... -0.04732466  0.02775996\n",
      "    -0.04439557]\n",
      "   ...\n",
      "   [-0.01982365 -0.01791743 -0.02720642 ...  0.04374894  0.00593596\n",
      "    -0.03696255]\n",
      "   [ 0.04977195 -0.04734489  0.03603316 ...  0.02147819 -0.01079581\n",
      "     0.01098774]\n",
      "   [-0.02969257 -0.04470747 -0.01297478 ...  0.00425569  0.04649167\n",
      "     0.01845345]]\n",
      "\n",
      "  [[-0.01118176 -0.02968656 -0.01801552 ...  0.01043593 -0.03469091\n",
      "    -0.02980684]\n",
      "   [-0.04672707 -0.04060597 -0.020531   ...  0.01239122 -0.02669505\n",
      "     0.00679113]\n",
      "   [-0.01513953 -0.03032892 -0.02783594 ...  0.02081538 -0.00141674\n",
      "    -0.02812806]\n",
      "   ...\n",
      "   [ 0.03971856  0.04410468  0.0388794  ... -0.01135701  0.01432887\n",
      "    -0.01839194]\n",
      "   [ 0.0160781   0.01620979  0.00792317 ... -0.01133643 -0.04866629\n",
      "     0.01791343]\n",
      "   [-0.01845495  0.03533474  0.03007039 ... -0.02640723  0.03847486\n",
      "    -0.0264535 ]]\n",
      "\n",
      "  [[-0.03231049  0.03619111  0.03282149 ...  0.03519907  0.00908076\n",
      "    -0.01544927]\n",
      "   [ 0.00707614  0.04906603 -0.01784249 ... -0.00073689  0.02378707\n",
      "     0.02068204]\n",
      "   [-0.02421255  0.00851476  0.003128   ...  0.04040729  0.00418914\n",
      "     0.02163072]\n",
      "   ...\n",
      "   [ 0.00883468 -0.03171302  0.03936528 ...  0.01440836  0.0225472\n",
      "     0.01837451]\n",
      "   [ 0.02471367 -0.03332292  0.0424684  ...  0.04357716 -0.03137299\n",
      "     0.02726717]\n",
      "   [ 0.00893863 -0.01112735  0.01290475 ... -0.02347318  0.01318434\n",
      "     0.04235914]]\n",
      "\n",
      "  [[ 0.03075189  0.01445604 -0.01687276 ...  0.04464692 -0.00812354\n",
      "     0.03265326]\n",
      "   [ 0.03479401  0.04674525  0.0359485  ...  0.03555309  0.03870037\n",
      "    -0.00184059]\n",
      "   [-0.03707618  0.03059775 -0.02512542 ...  0.00220569 -0.00923418\n",
      "     0.0273133 ]\n",
      "   ...\n",
      "   [-0.03411472  0.0272862  -0.02761677 ... -0.03209342  0.01457502\n",
      "     0.03530506]\n",
      "   [-0.01655326  0.00206479 -0.02610011 ... -0.00635348  0.00347515\n",
      "     0.00232338]\n",
      "   [-0.00150543 -0.0035369   0.01127417 ...  0.04058098 -0.03747934\n",
      "     0.04748492]]]\n",
      "\n",
      "\n",
      " [[[-0.04641698  0.00314027 -0.03429294 ...  0.048417    0.00369896\n",
      "     0.03814713]\n",
      "   [-0.04941765  0.04324081 -0.0218429  ... -0.04826056  0.03324436\n",
      "    -0.0016489 ]\n",
      "   [-0.02398826 -0.04725816  0.04492328 ...  0.03614036 -0.0028008\n",
      "    -0.0104395 ]\n",
      "   ...\n",
      "   [ 0.00847607  0.02030182 -0.01855414 ... -0.03124629 -0.02139722\n",
      "    -0.02384311]\n",
      "   [ 0.04304827 -0.00062686  0.02635021 ... -0.04326535  0.03798283\n",
      "    -0.04181317]\n",
      "   [-0.01732963 -0.01505212  0.0475791  ...  0.02230355 -0.0181684\n",
      "     0.01600151]]\n",
      "\n",
      "  [[ 0.03061612 -0.0341927   0.00098901 ... -0.02560197 -0.03192209\n",
      "     0.00337461]\n",
      "   [-0.02790656 -0.03618253  0.01336506 ... -0.00751833  0.0317372\n",
      "     0.0352536 ]\n",
      "   [-0.031029   -0.00155584  0.00374106 ...  0.04691182 -0.02655953\n",
      "    -0.03620771]\n",
      "   ...\n",
      "   [ 0.04556498  0.00084997 -0.04986092 ... -0.01850628 -0.01954835\n",
      "    -0.00783192]\n",
      "   [-0.03551173 -0.04598298  0.03431802 ... -0.04041164 -0.01438118\n",
      "     0.03431535]\n",
      "   [ 0.03507736  0.03120804 -0.00849738 ...  0.03980675  0.03565464\n",
      "    -0.00201871]]\n",
      "\n",
      "  [[-0.01582593  0.01857598 -0.01022607 ...  0.00126004  0.01568503\n",
      "     0.02476365]\n",
      "   [-0.02137592 -0.02361871 -0.00628151 ...  0.04371483  0.03170813\n",
      "    -0.02073337]\n",
      "   [-0.01555707  0.01243065 -0.02150948 ...  0.01449725 -0.01780314\n",
      "    -0.01901211]\n",
      "   ...\n",
      "   [-0.04471185  0.03084158  0.01281628 ... -0.04235821 -0.04756716\n",
      "     0.04937163]\n",
      "   [-0.04281552 -0.0407881   0.01859481 ...  0.0109788  -0.04634551\n",
      "    -0.0417434 ]\n",
      "   [ 0.04407356 -0.03488468 -0.00808454 ...  0.04517764 -0.01279107\n",
      "    -0.00705668]]\n",
      "\n",
      "  [[-0.02854068 -0.00805043 -0.0272987  ... -0.00803077  0.01895955\n",
      "     0.02770711]\n",
      "   [-0.0372714   0.01784055 -0.03262285 ... -0.00136745  0.02755835\n",
      "    -0.00539409]\n",
      "   [ 0.02497635 -0.00270193  0.008428   ...  0.00437149  0.03781258\n",
      "    -0.04049527]\n",
      "   ...\n",
      "   [-0.0293591   0.02809603  0.0491435  ...  0.02817115 -0.0453805\n",
      "    -0.01145207]\n",
      "   [ 0.0095055  -0.017925   -0.0013053  ...  0.03963119  0.01961777\n",
      "    -0.03200077]\n",
      "   [ 0.01549014  0.00731641  0.040658   ...  0.01892545  0.0195655\n",
      "     0.02941949]]\n",
      "\n",
      "  [[-0.00989971 -0.01930555  0.00197548 ...  0.0412341  -0.01381008\n",
      "    -0.03801344]\n",
      "   [-0.02928876  0.04942412  0.03630866 ... -0.02152727  0.02551932\n",
      "     0.03401986]\n",
      "   [-0.00292008  0.0199351  -0.03896729 ... -0.00831064  0.03966839\n",
      "     0.03380677]\n",
      "   ...\n",
      "   [-0.03937515  0.02678038  0.01605412 ...  0.03853501  0.02446279\n",
      "     0.01952814]\n",
      "   [-0.02926554 -0.02920498 -0.02949852 ...  0.02127263  0.00666757\n",
      "    -0.03250767]\n",
      "   [ 0.03661202  0.01614572 -0.02860532 ...  0.02665582 -0.04652996\n",
      "    -0.0185696 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.02695308 -0.04586656  0.02167412 ... -0.00263732 -0.01542549\n",
      "    -0.04987793]\n",
      "   [-0.00169749  0.02743342  0.03265119 ...  0.02135069 -0.00385584\n",
      "    -0.03456135]\n",
      "   [-0.04234121 -0.04560274  0.03740444 ...  0.01527352 -0.04634574\n",
      "     0.04303903]\n",
      "   ...\n",
      "   [-0.03402893 -0.04051281  0.02621138 ...  0.00118377 -0.04690784\n",
      "     0.0091136 ]\n",
      "   [ 0.01453037  0.02600156  0.01120242 ... -0.03574011  0.04075814\n",
      "     0.03073308]\n",
      "   [ 0.01534027 -0.03432741  0.00460301 ... -0.02019762  0.02748041\n",
      "    -0.0412472 ]]\n",
      "\n",
      "  [[ 0.03546058 -0.03513705  0.04716296 ... -0.046253   -0.04554559\n",
      "     0.02391272]\n",
      "   [-0.04019383 -0.04511644 -0.00406075 ...  0.01104588  0.01711488\n",
      "    -0.02895185]\n",
      "   [ 0.01662413 -0.01645452 -0.03074492 ... -0.00961056  0.03307754\n",
      "    -0.03467237]\n",
      "   ...\n",
      "   [ 0.00909286  0.03841453  0.03237244 ... -0.04779918 -0.02303717\n",
      "    -0.04605169]\n",
      "   [-0.02210854  0.04581708  0.01728943 ...  0.03463403  0.00727842\n",
      "    -0.04084612]\n",
      "   [-0.04405653  0.04725871  0.04113246 ...  0.02594726 -0.04555267\n",
      "     0.02369091]]\n",
      "\n",
      "  [[ 0.04639901 -0.0012993   0.01313135 ... -0.03421402 -0.02972853\n",
      "    -0.00536472]\n",
      "   [-0.02140943 -0.036727   -0.03236983 ...  0.03217748  0.04623724\n",
      "    -0.04732917]\n",
      "   [-0.01141088  0.03958363  0.03452751 ...  0.03658186  0.0441038\n",
      "     0.03093176]\n",
      "   ...\n",
      "   [-0.00701078  0.04240293  0.0416933  ...  0.00593919  0.01923508\n",
      "    -0.03187414]\n",
      "   [ 0.01647128  0.0380423  -0.00827285 ...  0.02624648 -0.04884442\n",
      "     0.04834852]\n",
      "   [-0.04395336  0.01883173  0.04178885 ... -0.02688478 -0.01810045\n",
      "     0.03209056]]\n",
      "\n",
      "  [[ 0.0177127  -0.01862056  0.04692079 ... -0.04722177  0.01270304\n",
      "    -0.03135116]\n",
      "   [-0.02596567 -0.0369135   0.04840152 ...  0.02055709 -0.01673775\n",
      "    -0.04230272]\n",
      "   [-0.02211776  0.00513694  0.01077489 ...  0.00099412  0.03920997\n",
      "     0.00149355]\n",
      "   ...\n",
      "   [ 0.0360711   0.01708814 -0.00222402 ... -0.01998712  0.00946305\n",
      "    -0.0251416 ]\n",
      "   [ 0.03143337 -0.03397809  0.02101913 ... -0.03368418  0.00118482\n",
      "    -0.04203346]\n",
      "   [-0.03859023  0.02010666 -0.00918356 ... -0.02040586  0.01608725\n",
      "    -0.01110042]]\n",
      "\n",
      "  [[ 0.01831097  0.0447978  -0.03737563 ...  0.02284792 -0.03852087\n",
      "     0.0136166 ]\n",
      "   [-0.01971607 -0.01998023  0.00092485 ... -0.028234    0.00581669\n",
      "    -0.04237927]\n",
      "   [ 0.01237319 -0.03513061 -0.0331844  ...  0.00777828  0.01566911\n",
      "    -0.0348357 ]\n",
      "   ...\n",
      "   [-0.01229731  0.0197227   0.00125445 ... -0.03833681 -0.04973624\n",
      "     0.0022627 ]\n",
      "   [-0.04529316  0.00120697  0.03969873 ...  0.03696221  0.01735859\n",
      "     0.0002128 ]\n",
      "   [ 0.01233965  0.03564361  0.03647638 ...  0.00699311 -0.01761245\n",
      "    -0.03877518]]]\n",
      "\n",
      "\n",
      " [[[ 0.00210119 -0.03866072  0.01052982 ... -0.03612137  0.00353507\n",
      "     0.00020979]\n",
      "   [ 0.0473831   0.02336487 -0.01489074 ... -0.04398214  0.00726961\n",
      "     0.02028793]\n",
      "   [-0.00199445  0.01604242  0.04840696 ... -0.00621194 -0.03309612\n",
      "    -0.00624623]\n",
      "   ...\n",
      "   [-0.00441387  0.0044953  -0.02295207 ...  0.00391088  0.01331523\n",
      "    -0.0096722 ]\n",
      "   [ 0.02515772  0.0130159   0.04423243 ...  0.0177328   0.037655\n",
      "     0.02956108]\n",
      "   [-0.03057973 -0.02849448 -0.01819744 ... -0.00815318  0.00284134\n",
      "    -0.03346335]]\n",
      "\n",
      "  [[ 0.01136422 -0.00202979 -0.01993427 ...  0.0094265  -0.00254426\n",
      "     0.01421324]\n",
      "   [-0.02751525 -0.04454924 -0.00975496 ...  0.02768365  0.01998277\n",
      "     0.00519265]\n",
      "   [-0.04738351 -0.04867504 -0.03441076 ... -0.04474207  0.02909709\n",
      "     0.04917197]\n",
      "   ...\n",
      "   [-0.03429412 -0.0380128   0.04983953 ... -0.03763289  0.03724014\n",
      "     0.01169682]\n",
      "   [ 0.02988893 -0.04042019  0.04595118 ... -0.04693388 -0.01352514\n",
      "     0.00765783]\n",
      "   [-0.02624935 -0.00395392  0.01922032 ...  0.00904818 -0.01552862\n",
      "     0.0301136 ]]\n",
      "\n",
      "  [[-0.03674684 -0.03667412  0.00466882 ... -0.01855363 -0.01260101\n",
      "    -0.02420232]\n",
      "   [-0.01114066 -0.0212872  -0.0005864  ... -0.0421037  -0.02026761\n",
      "    -0.04349008]\n",
      "   [-0.00448587  0.04971689 -0.01204653 ... -0.01647191 -0.02880074\n",
      "    -0.03127275]\n",
      "   ...\n",
      "   [ 0.0208859   0.04192629  0.02114678 ... -0.04813154  0.01301526\n",
      "    -0.04293585]\n",
      "   [-0.02327302  0.00681122 -0.02272156 ...  0.00324496  0.03712713\n",
      "    -0.00488291]\n",
      "   [ 0.02555377 -0.04422366  0.01045742 ...  0.03828343  0.03794814\n",
      "     0.04998127]]\n",
      "\n",
      "  [[-0.01969211  0.01019784  0.00607885 ...  0.03551546  0.03548541\n",
      "     0.03478429]\n",
      "   [-0.02777141 -0.04092852  0.04204762 ... -0.01317928 -0.0297695\n",
      "    -0.02660847]\n",
      "   [-0.03742688 -0.04742626 -0.01409323 ...  0.00885551 -0.03892851\n",
      "     0.00371873]\n",
      "   ...\n",
      "   [ 0.04377096  0.00501794 -0.01720096 ... -0.04751101 -0.04094136\n",
      "    -0.0489387 ]\n",
      "   [-0.04771439 -0.00117277 -0.03942753 ... -0.04310885 -0.04267639\n",
      "    -0.03887916]\n",
      "   [-0.00155123 -0.00194237 -0.01851851 ...  0.01899127  0.04596362\n",
      "    -0.00566851]]\n",
      "\n",
      "  [[ 0.02457872 -0.02640228 -0.02569126 ...  0.04439083  0.00710184\n",
      "     0.04583414]\n",
      "   [-0.02307321 -0.0037833  -0.03981531 ... -0.00488945  0.00099013\n",
      "    -0.04591533]\n",
      "   [ 0.02828758  0.04938019 -0.04902786 ... -0.01724614 -0.03879223\n",
      "    -0.01681564]\n",
      "   ...\n",
      "   [-0.04452438  0.02958787 -0.02748168 ... -0.01243705  0.04421413\n",
      "     0.03036593]\n",
      "   [ 0.02427549  0.02533993 -0.04730317 ...  0.04530232 -0.02183769\n",
      "     0.00423502]\n",
      "   [-0.01234174  0.02574463  0.04721269 ...  0.03515344 -0.00642775\n",
      "    -0.00726675]]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    total_batch = int(len(y_train) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x = X_train[i*batch_size:(i+1)*batch_size,:]/256\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y_one_hot = np.zeros([batch_size,10])\n",
    "            for j in range(len(batch_y)):\n",
    "                batch_y_one_hot[j,int(batch_y[j])]=1\n",
    "            _, c = sess.run([optimizer, cross_entropy], feed_dict={x: batch_x, y: batch_y_one_hot})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "    \n",
    "    y_test_one_hot = np.zeros([len(y_test),10])\n",
    "    for j in range(len(y_test)):\n",
    "        y_test_one_hot[j,int(y_test[j])]=1\n",
    "    print(sess.run(accuracy, feed_dict={x: X_test, y: y_test_one_hot}))\n",
    "    \n",
    "    #names = [op.name for op in graph.get_operations() if op.type=='Conv2D']\n",
    "    #[var.name for var in tf.trainable_variables()]\n",
    "    var2 = [v for v in tf.trainable_variables() if v.name == 'conv2d_1/kernel:0'][0]\n",
    "    v = sess.run(var2)\n",
    "    #print(var2)\n",
    "    #print(v)\n",
    "    \n",
    "#0.988! \n",
    "#0.994 (5 ep) with augmentation but somehow i did not shuffle\n",
    "#0.977 (1 ep) with augmentation but somehow i never had the loss line\n",
    "#0.993 with augmentation and 10 epochs and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow advanced with estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see file: tf_tutorial_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow with Keras\n",
    "http://localhost:8888/notebooks/Google%20Drive/AI_Residency/PythonNotebooks/TensorFlow-Tutorials/03C_Keras_API.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True)\n",
    "data.test.cls = np.argmax(data.test.labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import InputLayer, Input,Reshape, MaxPooling2D, Conv2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "img_shape_full = (img_size, img_size, 1)\n",
    "num_channels = 1\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(img_size_flat,))\n",
    "net = inputs\n",
    "net = Reshape(img_shape_full)(net)\n",
    "net = Conv2D(kernel_size=5, strides=1, filters=16, padding='same',\n",
    "             activation='relu', name='layer_conv1')(net)\n",
    "net = MaxPooling2D(pool_size=2, strides=2)(net)\n",
    "net = Conv2D(kernel_size=5, strides=1, filters=36, padding='same',\n",
    "             activation='relu', name='layer_conv2')(net)\n",
    "net = MaxPooling2D(pool_size=2, strides=2)(net)\n",
    "net = Flatten()(net)\n",
    "net = Dense(128, activation='relu')(net)\n",
    "net = Dense(num_classes, activation='softmax')(net)\n",
    "outputs = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model(inputs=inputs, outputs=outputs)\n",
    "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 37s 677us/step - loss: 0.2021 - acc: 0.9372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1a4a1aaa58>"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x=data.train.images,y=data.train.labels,epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 279us/step\n"
     ]
    }
   ],
   "source": [
    "result = model2.evaluate(x=data.test.images,y=data.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.06103903693924658\n",
      "acc 0.9802\n",
      "acc: 98.02%\n"
     ]
    }
   ],
   "source": [
    "for name, value in zip(model2.metrics_names, result):\n",
    "    print(name, value)\n",
    "print(\"{0}: {1:.2%}\".format(model2.metrics_names[1], result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(x=data.test.images)\n",
    "cls_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = 'model.keras'\n",
    "model2.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with tensorboard somehow not working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kohstall/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-1-fe10fce4150a>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 0.601\n",
      "Epoch: 2 cost = 0.219\n",
      "Epoch: 3 cost = 0.156\n",
      "Epoch: 4 cost = 0.125\n",
      "Epoch: 5 cost = 0.102\n",
      "0.9761\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.5\n",
    "epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "n_L0 = 784\n",
    "n_L1 = 300\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None,n_L0])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_L0, n_L1], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([n_L1]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([n_L1, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "\n",
    "L1_x = tf.add(tf.matmul(x, W1), b1)\n",
    "L1_z = tf.nn.relu(L1_x)\n",
    "\n",
    "L2_x = tf.add(tf.matmul(L1_z, W2), b2)\n",
    "y_ = tf.nn.softmax(L2_x)\n",
    "\n",
    "y_clipped = tf.clip_by_value(y_ , 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)+ (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "logs_path = '/tmp/tensorflow_logs/MYexample/'\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "tf.summary.scalar(\"accuracy\", tf.reduce_mean(tf.cast(accuracy, tf.float32)))\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _, c, summary = sess.run([optimiser, cross_entropy, merged_summary_op], feed_dict={x: batch_x, y: batch_y})\n",
    "            #_, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "        \n",
    "        \n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "    \n",
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "\n",
    "\n",
    "#this shit only works once and then kernel needs to be restarted\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kohstall/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-1-bcc0889a48f4>:16: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/kohstall/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-bcc0889a48f4>:74: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Summary name W1:0 is illegal; using W1_0 instead.\n",
      "INFO:tensorflow:Summary name W2:0 is illegal; using W2_0 instead.\n",
      "INFO:tensorflow:Summary name W3:0 is illegal; using W3_0 instead.\n",
      "INFO:tensorflow:Summary name b1:0 is illegal; using b1_0 instead.\n",
      "INFO:tensorflow:Summary name b2:0 is illegal; using b2_0 instead.\n",
      "INFO:tensorflow:Summary name b3:0 is illegal; using b3_0 instead.\n",
      "INFO:tensorflow:Summary name W1:0/gradient is illegal; using W1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name W2:0/gradient is illegal; using W2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name W3:0/gradient is illegal; using W3_0/gradient instead.\n",
      "INFO:tensorflow:Summary name b1:0/gradient is illegal; using b1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name b2:0/gradient is illegal; using b2_0/gradient instead.\n",
      "INFO:tensorflow:Summary name b3:0/gradient is illegal; using b3_0/gradient instead.\n",
      "Epoch: 0001 cost= 59.836136659\n",
      "Epoch: 0002 cost= 13.947095534\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9129\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Graph and Loss visualization using Tensorboard.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 2\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "logs_path = '/tmp/tensorflow_logs/MYexampleAdv/'\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph Input\n",
    "# mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition => 10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Create a summary to visualize the first layer ReLU activation\n",
    "    tf.summary.histogram(\"relu1\", layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Create another summary to visualize the second layer ReLU activation\n",
    "    tf.summary.histogram(\"relu2\", layer_2)\n",
    "    # Output layer\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'w1': tf.Variable(tf.random_normal([n_input, n_hidden_1]), name='W1'),\n",
    "    'w2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name='W2'),\n",
    "    'w3': tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name='W3')\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1]), name='b1'),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2]), name='b2'),\n",
    "    'b3': tf.Variable(tf.random_normal([n_classes]), name='b3')\n",
    "}\n",
    "\n",
    "# Encapsulating all ops into scopes, making Tensorboard's Graph\n",
    "# Visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Build model\n",
    "    pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    # Softmax Cross entropy (cost function)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Op to calculate every variable gradient\n",
    "    grads = tf.gradients(loss, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    # Op to update all variables according to their gradient\n",
    "    apply_grads = optimizer.apply_gradients(grads_and_vars=grads)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", acc)\n",
    "# Create summaries to visualize weights\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.name, var)\n",
    "# Summarize all gradients\n",
    "for grad, var in grads:\n",
    "    tf.summary.histogram(var.name + '/gradient', grad)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path,\n",
    "                                            graph=tf.get_default_graph())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([apply_grads, loss, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train save restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "model_path = \"/tmp/model.ckpt\"\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Running first session\n",
    "print(\"Starting 1st session...\")\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(3):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"First Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "# Running a new session\n",
    "print(\"Starting 2nd session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "    # Resume training\n",
    "    for epoch in range(7):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Second Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval(\n",
    "        {x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-388-2eadab7e09c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SalePrice'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredicted_prices\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# you could use any filename. We choose submission here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission.to_csv('submission.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_test, verbose=0)\n",
    "\n",
    "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
    "                         \"Label\": predictions})\n",
    "submissions.to_csv(\"DR.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[var.name for var in tf.trainable_variables()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
