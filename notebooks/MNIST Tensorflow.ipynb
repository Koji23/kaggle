{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home = './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display(mnist)\n",
    "#display(mnist.data)\n",
    "#display(mnist.target)\n",
    "display(mnist.data.shape)\n",
    "display(mnist.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mnist.data\n",
    "target = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65a8d3e160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[0,:].reshape([28,28]), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples, n_pix = data.shape\n",
    "n_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "dataL = np.zeros([n_samples*5,n_pix])\n",
    "targetL = np.zeros([n_samples*5])\n",
    "index=0\n",
    "for i in range(n_samples):\n",
    "    for j in range(-2,3):\n",
    "        #im = Image.fromarray(np.uint8(data[0,:].reshape([28,28])))\n",
    "        dataL[index,:] = np.array( Image.fromarray(data[i,:].reshape([28,28])).rotate(j*5) ).ravel()\n",
    "        targetL[index]=target[i]\n",
    "        index +=1\n",
    "        \n",
    "#im = Image.fromarray(data[0,:].reshape([28,28]))\n",
    "#imAr = np.array( im.transform(im.size, Image.AFFINE, (1, 1, 0, 1, 1, 0)) ).ravel()\n",
    "\n",
    "display(dataL.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f659d146978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data.ravel(), bins =256)\n",
    "plt.ylim([0,1e6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f659d146f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(target, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f659ce23240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "data, target = shuffle(data,target, random_state=0)\n",
    "#dataL, targetL = shuffle(dataL,targetL, random_state=0)\n",
    "\n",
    "\n",
    "plt.plot(target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6300, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(dataL, targetL, test_size=0.1, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[0:n_samples//10,:], target[0:n_samples//10], test_size=0.1, random_state=42)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 3., 6.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.predict(X_test[0:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.predict_proba(X_test[0:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9442857142857143"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(70, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(70,30))\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 3., 6.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(X_test[0:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8614285714285714"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test,y_test)\n",
    "#got 0.958 right out of the box with MLPclassifier\n",
    "#got 0.970 with augmentation of plus minus 2*10degree of rotating\n",
    "#got 0.966 with hidden_layer_sizes=(100,20,)   , no augmentation\n",
    "#for much smaller dataset / 10   I get 0.91 on default and only 0.75 for hidden_layer_sizes=(100,20,) (not enough data to train right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow basics\n",
    "http://adventuresinmachinelearning.com/python-tensorflow-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable a is [[ 3.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [15.]\n",
      " [18.]\n",
      " [21.]\n",
      " [24.]\n",
      " [27.]\n",
      " [30.]]\n"
     ]
    }
   ],
   "source": [
    "const = tf.constant(2.0, name='const')\n",
    "#b = tf.Variable(2.0, name='b')\n",
    "b = tf.placeholder(tf.float32, [None, 1], name='b')\n",
    "c = tf.Variable(1.0, name='c')\n",
    "d = tf.add(b,c, name='d')\n",
    "e = tf.add(c, const, name='e')\n",
    "a = tf.multiply(d,e, name='a')\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    #a_out=sess.run(a)\n",
    "    a_out = sess.run(a, feed_dict={b: np.arange(0, 10)[:, np.newaxis]})\n",
    "    print('Variable a is {}'.format(a_out))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow solution with tensorflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-1f7b200028f2>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: 1 cost = 0.531\n",
      "Epoch: 2 cost = 0.202\n",
      "0.9693\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.5\n",
    "epochs = 2\n",
    "batch_size = 100\n",
    "\n",
    "n_L0 = 784\n",
    "n_L1 = 300\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None,n_L0])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_L0, n_L1], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([n_L1]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([n_L1, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "\n",
    "L1_x = tf.add(tf.matmul(x, W1), b1)\n",
    "L1_z = tf.nn.relu(L1_x)\n",
    "\n",
    "L2_x = tf.add(tf.matmul(L1_z, W2), b2)\n",
    "y_ = tf.nn.softmax(L2_x)\n",
    "\n",
    "y_clipped = tf.clip_by_value(y_ , 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)+ (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "   # initialise the variables\n",
    "   sess.run(init_op)\n",
    "   total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "   for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "   print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow with original data from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "n_L0 = 784\n",
    "n_L1 = 300\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_L0])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "#yV = tf.placeholder(tf.float32, [None])\n",
    "#y = tf.one_hot(tf.cast(yV,tf.int32),10)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_L0, n_L1], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([n_L1]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([n_L1, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "\n",
    "L1_x = tf.add(tf.matmul(x, W1), b1)\n",
    "L1_z = tf.nn.relu(L1_x)\n",
    "\n",
    "L2_x = tf.add(tf.matmul(L1_z, W2), b2)\n",
    "y_ = tf.nn.softmax(L2_x)\n",
    "\n",
    "y_clipped = tf.clip_by_value(y_ , 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)+ (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 1.825\n",
      "Epoch: 2 cost = 0.690\n",
      "Epoch: 3 cost = 0.421\n",
      "Epoch: 4 cost = 0.307\n",
      "Epoch: 5 cost = 0.234\n",
      "Epoch: 6 cost = 0.180\n",
      "Epoch: 7 cost = 0.140\n",
      "Epoch: 8 cost = 0.108\n",
      "Epoch: 9 cost = 0.085\n",
      "Epoch: 10 cost = 0.068\n",
      "0.9242857\n"
     ]
    }
   ],
   "source": [
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    total_batch = int(len(y_train) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x = X_train[i*batch_size:(i+1)*batch_size,:]/256\n",
    "            #batch_y = tf.one_hot(y_train[i*batch_size:(i+1)*batch_size],10)\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y_one_hot = np.zeros([batch_size,10])\n",
    "            for j in range(len(batch_y)):\n",
    "                batch_y_one_hot[j,int(batch_y[j])]=1\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y_one_hot})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "    \n",
    "    y_test_one_hot = np.zeros([len(y_test),10])\n",
    "    for j in range(len(y_test)):\n",
    "        y_test_one_hot[j,int(y_test[j])]=1\n",
    "    print(sess.run(accuracy, feed_dict={x: X_test, y: y_test_one_hot}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 3 layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "n_L0 = 784\n",
    "n_L1 = 50\n",
    "n_L2 = 20\n",
    "n_L3 = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_L0])\n",
    "#y = tf.placeholder(tf.float32, [None, 10])\n",
    "yV = tf.placeholder(tf.float32, [None])\n",
    "y = tf.one_hot(tf.cast(yV,tf.int32),10)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_L0, n_L1], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([n_L1]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([n_L1, n_L2], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([n_L2]), name='b2')\n",
    "W3 = tf.Variable(tf.random_normal([n_L2, n_L3], stddev=0.03), name='W3')\n",
    "b3 = tf.Variable(tf.random_normal([n_L3]), name='b3')\n",
    "\n",
    "L1_x = tf.add(tf.matmul(x, W1), b1)\n",
    "L1_z = tf.nn.relu(L1_x)\n",
    "\n",
    "L2_x = tf.add(tf.matmul(L1_z, W2), b2)\n",
    "L2_z = tf.nn.relu(L2_x)\n",
    "\n",
    "L3_x = tf.add(tf.matmul(L2_z, W3), b3)\n",
    "y_ = tf.nn.softmax(L3_x)\n",
    "\n",
    "y_clipped = tf.clip_by_value(y_ , 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)+ (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 2.802\n",
      "Epoch: 2 cost = 1.706\n",
      "Epoch: 3 cost = 0.978\n",
      "Epoch: 4 cost = 0.861\n",
      "Epoch: 5 cost = 0.547\n",
      "Epoch: 6 cost = 0.420\n",
      "Epoch: 7 cost = 0.366\n",
      "Epoch: 8 cost = 0.362\n",
      "Epoch: 9 cost = 0.266\n",
      "Epoch: 10 cost = 0.281\n",
      "0.8857143\n"
     ]
    }
   ],
   "source": [
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    total_batch = int(len(y_train) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x = X_train[i*batch_size:(i+1)*batch_size,:]/256\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y_one_hot = np.zeros([batch_size,10])\n",
    "            for j in range(len(batch_y)):\n",
    "                batch_y_one_hot[j,int(batch_y[j])]=1\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, yV: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "    \n",
    "    y_test_one_hot = np.zeros([len(y_test),10])\n",
    "    for j in range(len(y_test)):\n",
    "        y_test_one_hot[j,int(y_test[j])]=1\n",
    "    print(sess.run(accuracy, feed_dict={x: X_test, yV: y_test}))\n",
    "    \n",
    "#maybe too many parameters and we need to train longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow simple 2 layer use layers.dense and built in entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "n_L0 = 784\n",
    "n_L1 = 300\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_L0])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "L1_z = tf.layers.dense(inputs=x,    units=300, activation=tf.nn.relu)\n",
    "L2_x = tf.layers.dense(inputs=L1_z, units=10)\n",
    "\n",
    "y_ = tf.nn.softmax(L2_x)\n",
    "\n",
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=tf.argmax(input=y, axis=1), logits=L2_x)\n",
    "    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 0.714\n",
      "Epoch: 2 cost = 0.295\n",
      "Epoch: 3 cost = 0.212\n",
      "Epoch: 4 cost = 0.160\n",
      "Epoch: 5 cost = 0.125\n",
      "Epoch: 6 cost = 0.098\n",
      "Epoch: 7 cost = 0.078\n",
      "Epoch: 8 cost = 0.063\n",
      "Epoch: 9 cost = 0.051\n",
      "Epoch: 10 cost = 0.041\n",
      "0.95857143\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(len(y_train) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x = X_train[i*batch_size:(i+1)*batch_size,:]/256\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y_one_hot = np.zeros([batch_size,10])\n",
    "            for j in range(len(batch_y)):\n",
    "                batch_y_one_hot[j,int(batch_y[j])]=1\n",
    "            _, c = sess.run([optimizer, cross_entropy], feed_dict={x: batch_x, y: batch_y_one_hot})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "    \n",
    "    y_test_one_hot = np.zeros([len(y_test),10])\n",
    "    for j in range(len(y_test)):\n",
    "        y_test_one_hot[j,int(y_test[j])]=1\n",
    "    print(sess.run(accuracy, feed_dict={x: X_test, y: y_test_one_hot}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow convolution nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "n_L0 = 784\n",
    "n_L1 = 300\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_L0])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#L1_z = tf.layers.dense(inputs=x,    units=300, activation=tf.nn.relu)\n",
    "#L2_x = tf.layers.dense(inputs=L1_z, units=10)\n",
    "\n",
    "input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=input_layer, filters=32,\n",
    "      kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu, name='conv1')\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=pool1, filters=64,\n",
    "      kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64]) # first dim corresponds to batch size\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "logits = tf.layers.dense(inputs=dense, units=10) #note there is no activation\n",
    "\n",
    "y_ = tf.nn.softmax(logits)\n",
    "\n",
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=tf.argmax(input=y, axis=1), logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy) # i forgot this line\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 2.290\n",
      "0.27285713\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    total_batch = int(len(y_train) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x = X_train[i*batch_size:(i+1)*batch_size,:]/256\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y_one_hot = np.zeros([batch_size,10])\n",
    "            for j in range(len(batch_y)):\n",
    "                batch_y_one_hot[j,int(batch_y[j])]=1\n",
    "            _, c = sess.run([optimizer, cross_entropy], feed_dict={x: batch_x, y: batch_y_one_hot})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "    \n",
    "    y_test_one_hot = np.zeros([len(y_test),10])\n",
    "    for j in range(len(y_test)):\n",
    "        y_test_one_hot[j,int(y_test[j])]=1\n",
    "    print(sess.run(accuracy, feed_dict={x: X_test, y: y_test_one_hot}))\n",
    "    \n",
    "    #names = [op.name for op in graph.get_operations() if op.type=='Conv2D']\n",
    "    #[var.name for var in tf.trainable_variables()]\n",
    "    var2 = [v for v in tf.trainable_variables() if v.name == 'conv2d_1/kernel:0']\n",
    "    v = sess.run(var2)\n",
    "    #print(var2)\n",
    "    #print(v)\n",
    "    \n",
    "#0.988! \n",
    "#0.994 (5 ep) with augmentation but somehow i did not shuffle\n",
    "#0.977 (1 ep) with augmentation but somehow i never had the loss line\n",
    "#0.993 with augmentation and 10 epochs and shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow advanced with estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see file: tf_tutorial_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow with Keras\n",
    "http://localhost:8888/notebooks/Google%20Drive/AI_Residency/PythonNotebooks/TensorFlow-Tutorials/03C_Keras_API.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True)\n",
    "data.test.cls = np.argmax(data.test.labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import InputLayer, Input,Reshape, MaxPooling2D, Conv2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "img_shape_full = (img_size, img_size, 1)\n",
    "num_channels = 1\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(img_size_flat,))\n",
    "net = inputs\n",
    "net = Reshape(img_shape_full)(net)\n",
    "net = Conv2D(kernel_size=5, strides=1, filters=16, padding='same',\n",
    "             activation='relu', name='layer_conv1')(net)\n",
    "net = MaxPooling2D(pool_size=2, strides=2)(net)\n",
    "net = Conv2D(kernel_size=5, strides=1, filters=36, padding='same',\n",
    "             activation='relu', name='layer_conv2')(net)\n",
    "net = MaxPooling2D(pool_size=2, strides=2)(net)\n",
    "net = Flatten()(net)\n",
    "net = Dense(128, activation='relu')(net)\n",
    "net = Dense(num_classes, activation='softmax')(net)\n",
    "outputs = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model(inputs=inputs, outputs=outputs)\n",
    "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 42s 765us/step - loss: 0.2042 - acc: 0.9367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f65a73bf8d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x=data.train.images,y=data.train.labels,epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 323us/step\n"
     ]
    }
   ],
   "source": [
    "result = model2.evaluate(x=data.test.images,y=data.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.051101757599134\n",
      "acc 0.9832\n",
      "acc: 98.32%\n"
     ]
    }
   ],
   "source": [
    "for name, value in zip(model2.metrics_names, result):\n",
    "    print(name, value)\n",
    "print(\"{0}: {1:.2%}\".format(model2.metrics_names[1], result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(x=data.test.images)\n",
    "cls_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = 'model.keras'\n",
    "model2.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with tensorboard somehow not working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 0.735\n",
      "Epoch: 2 cost = 0.258\n",
      "Epoch: 3 cost = 0.191\n",
      "Epoch: 4 cost = 0.157\n",
      "Epoch: 5 cost = 0.125\n",
      "0.969\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.5\n",
    "epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "n_L0 = 784\n",
    "n_L1 = 300\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None,n_L0])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_L0, n_L1], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([n_L1]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([n_L1, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "\n",
    "L1_x = tf.add(tf.matmul(x, W1), b1)\n",
    "L1_z = tf.nn.relu(L1_x)\n",
    "\n",
    "L2_x = tf.add(tf.matmul(L1_z, W2), b2)\n",
    "y_ = tf.nn.softmax(L2_x)\n",
    "\n",
    "y_clipped = tf.clip_by_value(y_ , 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)+ (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "logs_path = '/tmp/tensorflow_logs/MYexample/'\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "tf.summary.scalar(\"accuracy\", tf.reduce_mean(tf.cast(accuracy, tf.float32)))\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _, c, summary = sess.run([optimiser, cross_entropy, merged_summary_op], feed_dict={x: batch_x, y: batch_y})\n",
    "            #_, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "        \n",
    "        \n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "    \n",
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "\n",
    "\n",
    "#this shit only works once and then kernel needs to be restarted\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-43-bcc0889a48f4>:74: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "INFO:tensorflow:Summary name c:0 is illegal; using c_0 instead.\n",
      "INFO:tensorflow:Summary name W1:0 is illegal; using W1_0 instead.\n",
      "INFO:tensorflow:Summary name b1:0 is illegal; using b1_0 instead.\n",
      "INFO:tensorflow:Summary name W2:0 is illegal; using W2_0 instead.\n",
      "INFO:tensorflow:Summary name b2:0 is illegal; using b2_0 instead.\n",
      "INFO:tensorflow:Summary name W1_1:0 is illegal; using W1_1_0 instead.\n",
      "INFO:tensorflow:Summary name b1_1:0 is illegal; using b1_1_0 instead.\n",
      "INFO:tensorflow:Summary name W2_1:0 is illegal; using W2_1_0 instead.\n",
      "INFO:tensorflow:Summary name b2_1:0 is illegal; using b2_1_0 instead.\n",
      "INFO:tensorflow:Summary name W1_2:0 is illegal; using W1_2_0 instead.\n",
      "INFO:tensorflow:Summary name b1_2:0 is illegal; using b1_2_0 instead.\n",
      "INFO:tensorflow:Summary name W2_2:0 is illegal; using W2_2_0 instead.\n",
      "INFO:tensorflow:Summary name b2_2:0 is illegal; using b2_2_0 instead.\n",
      "INFO:tensorflow:Summary name W3:0 is illegal; using W3_0 instead.\n",
      "INFO:tensorflow:Summary name b3:0 is illegal; using b3_0 instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv1/kernel:0 is illegal; using conv1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv1/bias:0 is illegal; using conv1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv2d/kernel:0 is illegal; using conv2d/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0 is illegal; using conv2d/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0 is illegal; using dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0 is illegal; using dense_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_3/kernel:0 is illegal; using dense_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_3/bias:0 is illegal; using dense_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer_conv1/kernel:0 is illegal; using layer_conv1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer_conv1/bias:0 is illegal; using layer_conv1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer_conv2/kernel:0 is illegal; using layer_conv2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer_conv2/bias:0 is illegal; using layer_conv2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_4/kernel:0 is illegal; using dense_4/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_4/bias:0 is illegal; using dense_4/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_5/kernel:0 is illegal; using dense_5/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_5/bias:0 is illegal; using dense_5/bias_0 instead.\n",
      "INFO:tensorflow:Summary name RMSprop/lr:0 is illegal; using RMSprop/lr_0 instead.\n",
      "INFO:tensorflow:Summary name RMSprop/rho:0 is illegal; using RMSprop/rho_0 instead.\n",
      "INFO:tensorflow:Summary name RMSprop/decay:0 is illegal; using RMSprop/decay_0 instead.\n",
      "INFO:tensorflow:Summary name RMSprop/iterations:0 is illegal; using RMSprop/iterations_0 instead.\n",
      "trying to switch the dtype to  <dtype: 'float32'>  from  <dtype: 'int64'>\n",
      "INFO:tensorflow:Summary name training/RMSprop/Variable:0 is illegal; using training/RMSprop/Variable_0 instead.\n",
      "INFO:tensorflow:Summary name training/RMSprop/Variable_1:0 is illegal; using training/RMSprop/Variable_1_0 instead.\n",
      "INFO:tensorflow:Summary name training/RMSprop/Variable_2:0 is illegal; using training/RMSprop/Variable_2_0 instead.\n",
      "INFO:tensorflow:Summary name training/RMSprop/Variable_3:0 is illegal; using training/RMSprop/Variable_3_0 instead.\n",
      "INFO:tensorflow:Summary name training/RMSprop/Variable_4:0 is illegal; using training/RMSprop/Variable_4_0 instead.\n",
      "INFO:tensorflow:Summary name training/RMSprop/Variable_5:0 is illegal; using training/RMSprop/Variable_5_0 instead.\n",
      "INFO:tensorflow:Summary name training/RMSprop/Variable_6:0 is illegal; using training/RMSprop/Variable_6_0 instead.\n",
      "INFO:tensorflow:Summary name training/RMSprop/Variable_7:0 is illegal; using training/RMSprop/Variable_7_0 instead.\n",
      "INFO:tensorflow:Summary name W1_3:0 is illegal; using W1_3_0 instead.\n",
      "INFO:tensorflow:Summary name b1_3:0 is illegal; using b1_3_0 instead.\n",
      "INFO:tensorflow:Summary name W2_3:0 is illegal; using W2_3_0 instead.\n",
      "INFO:tensorflow:Summary name b2_3:0 is illegal; using b2_3_0 instead.\n",
      "INFO:tensorflow:Summary name W1_4:0 is illegal; using W1_4_0 instead.\n",
      "INFO:tensorflow:Summary name W2_4:0 is illegal; using W2_4_0 instead.\n",
      "INFO:tensorflow:Summary name W3_1:0 is illegal; using W3_1_0 instead.\n",
      "INFO:tensorflow:Summary name b1_4:0 is illegal; using b1_4_0 instead.\n",
      "INFO:tensorflow:Summary name b2_4:0 is illegal; using b2_4_0 instead.\n",
      "INFO:tensorflow:Summary name b3_1:0 is illegal; using b3_1_0 instead.\n",
      "INFO:tensorflow:Summary name c:0/gradient is illegal; using c_0/gradient instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tried to convert 'values' to a tensor and failed. Error: None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 214\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    523\u001b[0m               observed = ops.internal_convert_to_tensor(\n\u001b[0;32m--> 524\u001b[0;31m                   values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 214\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-bcc0889a48f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# Summarize all gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/gradient'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;31m# Merge all summaries into a single op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mmerged_summary_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/summary/summary.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(name, values, collections, family)\u001b[0m\n\u001b[1;32m    201\u001b[0m       default_name='HistogramSummary') as (tag, scope):\n\u001b[1;32m    202\u001b[0m     val = _gen_logging_ops.histogram_summary(\n\u001b[0;32m--> 203\u001b[0;31m         tag=tag, values=values, name=scope)\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0m_summary_op_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUMMARIES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py\u001b[0m in \u001b[0;36mhistogram_summary\u001b[0;34m(tag, values, name)\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 283\u001b[0;31m         \"HistogramSummary\", tag=tag, values=values, name=name)\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    526\u001b[0m               raise ValueError(\n\u001b[1;32m    527\u001b[0m                   \u001b[0;34m\"Tried to convert '%s' to a tensor and failed. Error: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                   (input_name, err))\n\u001b[0m\u001b[1;32m    529\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[1;32m    530\u001b[0m                       (input_name, op_type_name, observed))\n",
      "\u001b[0;31mValueError\u001b[0m: Tried to convert 'values' to a tensor and failed. Error: None values not supported."
     ]
    }
   ],
   "source": [
    "'''\n",
    "Graph and Loss visualization using Tensorboard.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 2\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "logs_path = '/tmp/tensorflow_logs/MYexampleAdv/'\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph Input\n",
    "# mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition => 10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Create a summary to visualize the first layer ReLU activation\n",
    "    tf.summary.histogram(\"relu1\", layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Create another summary to visualize the second layer ReLU activation\n",
    "    tf.summary.histogram(\"relu2\", layer_2)\n",
    "    # Output layer\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'w1': tf.Variable(tf.random_normal([n_input, n_hidden_1]), name='W1'),\n",
    "    'w2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name='W2'),\n",
    "    'w3': tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name='W3')\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1]), name='b1'),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2]), name='b2'),\n",
    "    'b3': tf.Variable(tf.random_normal([n_classes]), name='b3')\n",
    "}\n",
    "\n",
    "# Encapsulating all ops into scopes, making Tensorboard's Graph\n",
    "# Visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Build model\n",
    "    pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    # Softmax Cross entropy (cost function)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Op to calculate every variable gradient\n",
    "    grads = tf.gradients(loss, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    # Op to update all variables according to their gradient\n",
    "    apply_grads = optimizer.apply_gradients(grads_and_vars=grads)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", acc)\n",
    "# Create summaries to visualize weights\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.name, var)\n",
    "# Summarize all gradients\n",
    "for grad, var in grads:\n",
    "    tf.summary.histogram(var.name + '/gradient', grad)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path,\n",
    "                                            graph=tf.get_default_graph())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([apply_grads, loss, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train save restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Starting 1st session...\n",
      "Epoch: 0001 cost= 173.773194369\n",
      "Epoch: 0002 cost= 42.036626941\n",
      "Epoch: 0003 cost= 26.631709171\n",
      "First Optimization Finished!\n",
      "Accuracy: 0.9044\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Starting 2nd session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "Epoch: 0001 cost= 18.522173668\n",
      "Epoch: 0002 cost= 13.519820244\n",
      "Epoch: 0003 cost= 9.965822144\n",
      "Epoch: 0004 cost= 7.444633588\n",
      "Epoch: 0005 cost= 5.424372345\n",
      "Epoch: 0006 cost= 4.066486219\n",
      "Epoch: 0007 cost= 2.983093687\n",
      "Second Optimization Finished!\n",
      "Accuracy: 0.9355\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "model_path = \"/tmp/model.ckpt\"\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Running first session\n",
    "print(\"Starting 1st session...\")\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(3):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"First Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "# Running a new session\n",
    "print(\"Starting 2nd session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "    # Resume training\n",
    "    for epoch in range(7):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Second Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval(\n",
    "        {x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission.to_csv('submission.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_test, verbose=0)\n",
    "\n",
    "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
    "                         \"Label\": predictions})\n",
    "submissions.to_csv(\"DR.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[var.name for var in tf.trainable_variables()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
